{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Cleaning for Dry Bean Dataset\n",
                "\n",
                "This notebook cleans the `Dry_Bean_Dataset.xlsx` using custom functions and methodology from `clean_and_viz.ipynb`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import random as rand\n",
                "import string\n",
                "from difflib import get_close_matches\n",
                "\n",
                "# Ensure openpyxl is installed for reading Excel files\n",
                "# !pip install openpyxl"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define Custom Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a function to find outliers using IQR\n",
                "def find_outliers_IQR(df):\n",
                "    #this function takes a df or only a column and calculates outliers for each value based on IQR\n",
                "    \n",
                "    q1 = df.quantile(0.25) # variable for the first quartile\n",
                "    \n",
                "    q3 = df.quantile(0.75) #variable for the third quartile\n",
                "    \n",
                "    IQR = q3 - q1 #calculate the IQR\n",
                "    \n",
                "    outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))] #calculate outliers for every value and put them in a series\n",
                "    \n",
                "    return outliers\n",
                "\n",
                "\n",
                "# Range function\n",
                "def range_column(df):\n",
                "    max = df.max()\n",
                "    min = df.min()\n",
                "    range = max-min\n",
                "    return range\n",
                "\n",
                "\n",
                "\n",
                "# (NA/Null) Value Percentage Calculator\n",
                "def naValues(df):\n",
                "    \n",
                "    for column in df:\n",
                "        \n",
                "        total_values = len(df.index) # number of all values in a column\n",
                "        \n",
                "        total_garb = df[column].isna().sum() # number off all NA/Null values (apparently null and na are same in pandas, df.isnull() == df.isna())\n",
                "     \n",
                "        garb_perc = (total_garb * 100) // total_values\n",
                "        \n",
                "        print(column, \" has total of \", total_garb, \" NA/Null values\")\n",
                "        print(\"NA/Null percentage of \", column,\" is \", garb_perc, \"% \\n\")\n",
                "            \n",
                "\n",
                "#A custom function to replace special characters with corresponding letters\n",
                "def replaceSpecialChars(text):\n",
                "    # Define character replacements\n",
                "    char_replacements = {\n",
                "        '@': 'a',\n",
                "        '3': 'e',\n",
                "        '1': 'i',\n",
                "        '0': 'o',\n",
                "        '!': 'i',\n",
                "        '#': 'h',\n",
                "        '$': 's',\n",
                "        '5': 's',\n",
                "        '7': 't',\n",
                "        '9': 'g'\n",
                "    }\n",
                "    # If the input is not a string, return it as-is\n",
                "    if not isinstance(text, str):\n",
                "        return text\n",
                "    # remove empty spaces\n",
                "    text = text.strip()\n",
                "    # Replace each special character in the text\n",
                "    for char, replacement in char_replacements.items():\n",
                "        text = text.replace(char, replacement)\n",
                "    \n",
                "    return text\n",
                "        \n",
                "\n",
                "# A function that manipulates strings in columns.\n",
                "def str_basicclean(df,column ='',style = 'cap',sp_char = string.whitespace, sp_replace = False): # capitalizes strings and strips whitespaces by default, it can strip anything if provided as argument.\n",
                "    try:\n",
                "        if df[column].dtype == 'object':\n",
                "    \n",
                "            if type(sp_char) == str:\n",
                "                try:\n",
                "                    df[column] = df[column].str.strip()\n",
                "                    df[column] = df[column].str.strip(sp_char)\n",
                "                except:\n",
                "                    print('ERROR: Strip function did not work properly')\n",
                "            else:\n",
                "                df[column] = df[column].str.strip()\n",
                "            \n",
                "            if style == 'up':\n",
                "                df[column] = df[column].str.upper()\n",
                "            elif style == 'low':\n",
                "                df[column] = df[column].str.lower()\n",
                "            elif style == 'cap':\n",
                "                df[column] = df[column].str.capitalize()\n",
                "        \n",
                "            if sp_replace == True:\n",
                "                df[column] = df[column].apply(replaceSpecialChars)\n",
                "    except:\n",
                "        print(f'ERROR: Data Type of the {column} is not object.')\n",
                "        return 0\n",
                "    return df[column]\n",
                "\n",
                "\n",
                "\n",
                "#A function to list all the unique values\n",
                "def printUniqueValues(df, columnKey, sort_function=None):\n",
                "    \"\"\"\n",
                "    Print unique values of a column from a df, with optional sorting.\n",
                "\n",
                "    Parameters:\n",
                "    df (pd.df): The df containing the column.\n",
                "    columnKey (str): The key of the column whose unique values are to be printed.\n",
                "    sort_function (callable, optional): A function to sort the unique values. Defaults to None.\n",
                "    \"\"\"\n",
                "    unique_values = df[columnKey].unique().tolist()\n",
                "    \n",
                "    # Apply sorting if a sort function is provided\n",
                "    if sort_function:\n",
                "        unique_values = sorted(unique_values, key=sort_function)\n",
                "    \n",
                "    print(unique_values)\n",
                "\n",
                "#A custom function to fill the empty or nan values with a selected mod\n",
                "def num_filler(df, column = '', mode = 'median'):\n",
                "    if df[column].dtype == 'object':\n",
                "        print(f'ERROR: The data-type of column {column} is not numerical!')\n",
                "    else:\n",
                "        match mode:\n",
                "            case 'median':\n",
                "                df_median = df[column].median() \n",
                "                df[column] = df[column].fillna(df_median)\n",
                "                return df[column]\n",
                "            case 'mean':\n",
                "                df_mean = df[column].mean()\n",
                "                df[column] = df[column].fillna(df_mean)\n",
                "                return df[column]\n",
                "            case 'mod':\n",
                "                df_mod = df[column].mod()\n",
                "                df[column] = df[column].fillna(df_mod)\n",
                "                return df[column]\n",
                "            case 'prob':\n",
                "                value_counts = df[column].value_counts()\n",
                "                probabilities = value_counts / value_counts.sum()\n",
                "                \n",
                "                #Randomly generate values for NaN based on the probabilities\n",
                "                random_values = np.random.choice(value_counts.index, size=df[column].isna().sum(), p=probabilities)\n",
                "                \n",
                "                #Fill the empty values with the randomly generated values\n",
                "                df.loc[df[column].isna(), column] = random_values\n",
                "                return df[column]\n",
                "            case _:\n",
                "                print('ERROR: mod is unvalid. Please choose one of the mods below:\\n median(default), mean, mod, prob')\n",
                "                return df[column]\n",
                "\n",
                "def handle_incomplete_data(value, valid_values, default='Unknown', cutoff=0.4):\n",
                "    if not isinstance(value, str):\n",
                "        return default\n",
                "\n",
                "    # Strip and attempt to find a close match\n",
                "    value = value.strip()\n",
                "    matches = get_close_matches(value, valid_values, n=1, cutoff=cutoff)\n",
                "    if matches:\n",
                "        return matches[0]\n",
                "    else:\n",
                "        return default\n",
                "\n",
                "def process_column(df, column, valid_values, default='Unknown', cutoff=0.4):\n",
                "    \n",
                "    return df[column].apply(lambda x: handle_incomplete_data(x, valid_values, default, cutoff))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load the Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_excel('Dry_Bean_Dataset.xlsx')\n",
                "display(df.head())\n",
                "display(df.info())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check for Missing Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "naValues(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Cleaning Steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Clean Column Names\n",
                "df.columns = df.columns.str.strip()\n",
                "\n",
                "# 2. Identify potential numerical columns that might be object type due to dirtiness\n",
                "#    We assume string columns like 'Class' are categorical, others likely numerical.\n",
                "#    We'll try to convert everything to numeric, if it fails, we treat it as object/categorical.\n",
                "\n",
                "exclude_cols = ['Class'] # Presumed target variable\n",
                "\n",
                "for col in df.columns:\n",
                "    if col not in exclude_cols:\n",
                "        # Attempt to clean potential strings in numerical columns (e.g. \"12..\")\n",
                "        if df[col].dtype == 'object':\n",
                "             # Use replaceSpecialChars to fix things like '3' for 'e' if relevant, \n",
                "             # but for numbers we might just want to strip non-numeric chars or replacements.\n",
                "             # The reference notebook used replaceSpecialChars for specific text columns.\n",
                "             # Here we'll just try to force numeric conversion, handling errors.\n",
                "             \n",
                "             # Remove '...' if present as in the reference logic\n",
                "             df[col] = df[col].astype(str).str.replace('...', '', regex=False)\n",
                "             \n",
                "             # Try to convert to numeric\n",
                "             df[col] = pd.to_numeric(df[col], errors='coerce')\n",
                "             \n",
                "        # Fill missing values for numerical columns\n",
                "        if pd.api.types.is_numeric_dtype(df[col]):\n",
                "            num_filler(df, col, 'median')\n",
                "\n",
                "# 3. Clean Categorical/Object Columns\n",
                "for col in df.select_dtypes(include=['object']).columns:\n",
                "    # Basic string cleaning (strip, capitalize)\n",
                "    str_basicclean(df, col, style='cap', sp_replace=True)\n",
                "    # Replacements for special chars were applied above if sp_replace=True\n",
                "\n",
                "# Check info after cleaning\n",
                "df.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Outliers\n",
                "Detect and handle outliers using IQR method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                "for col in numeric_cols:\n",
                "    # Just displaying outliers count for now, handling depends on preference (cap, remove, ignore)\n",
                "    # The reference notebook function 'find_outliers_IQR' returns the outliers.\n",
                "    outliers = find_outliers_IQR(df[col])\n",
                "    print(f\"{col}: {len(outliers)} outliers found.\")\n",
                "    \n",
                "    # Optional: Fill or cap outliers? Reference notebook didn't explicitly show the 'fixing' part in the first 800 lines,\n",
                "    # but often they are capped or analyzed. We will leave them for now or ask user, \n",
                "    # but typically in cleaning we might want to handle them. \n",
                "    # Given instructions, we stick to the provided functions."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Cleaned Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_file = 'Dry_Bean_Dataset_Cleaned.csv'\n",
                "df.to_csv(output_file, index=False)\n",
                "print(f\"Cleaned dataset saved to {output_file}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}